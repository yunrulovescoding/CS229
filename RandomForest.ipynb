{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = pd.read_csv('music_data/train_X.csv')\n",
    "train_X = pd.read_csv('music_data/train_X_date_converted.csv')\n",
    "train_y = pd.read_csv('music_data/train_Y.csv')\n",
    "\n",
    "# val_X = pd.read_csv('music_data/valid_X.csv')\n",
    "val_X = pd.read_csv('music_data/valid_X_date_converted.csv')\n",
    "val_y = pd.read_csv('music_data/valid_Y.csv')\n",
    "\n",
    "# test_X = pd.read_csv('music_data/test_X.csv')\n",
    "test_X = pd.read_csv('music_data/test_X_date_converted.csv')\n",
    "test_y = pd.read_csv('music_data/test_Y.csv')\n",
    "\n",
    "# train_tiny_X = pd.read_csv('music_data/train_tiny_X.csv')\n",
    "train_tiny_X = pd.read_csv('music_data/train_tiny_X_date_converted.csv')\n",
    "train_tiny_y = pd.read_csv('music_data/train_tiny_Y.csv')\n",
    "\n",
    "# val_tiny_X = val_X[:1000].copy()\n",
    "val_tiny_X = pd.read_csv('music_data/val_tiny_X_date_converted.csv')\n",
    "val_tiny_y = val_y[:1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.660\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.69      0.71      4985\n",
      "         1.0       0.71      0.73      0.72      5015\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     10000\n",
      "   macro avg       0.71      0.71      0.71     10000\n",
      "weighted avg       0.71      0.71      0.71     10000\n",
      "\n",
      "0.7882886145975314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),\n",
    "                                               ('cat', categorical_transformer, catCols)])\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(penalty='l1'))])\n",
    "\n",
    "clf.fit(train_tiny_X, train_tiny_y['target'])\n",
    "print(\"model score: %.3f\" % clf.score(val_tiny_X, val_tiny_y['target']))\n",
    "print(classification_report(train_tiny_y['target'], clf.predict(train_tiny_X)))\n",
    "print(roc_auc_score(train_tiny_y['target'], clf.predict_proba(train_tiny_X)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = ['msno', 'song_id', 'source_screen_name', 'source_system_tab', 'source_type', 'genre_ids', 'artist_name', \n",
    "           'composer', 'lyricist', 'language', 'city', 'gender', 'registered_via']\n",
    "numCols = ['bd', 'song_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_from_ColumnTransformer(column_transformer):    \n",
    "    col_name = []\n",
    "    for transformer_in_columns in column_transformer.transformers_[:]:#the last transformer is ColumnTransformer's 'remainder'\n",
    "        raw_col_name = transformer_in_columns[2]\n",
    "        if isinstance(transformer_in_columns[1],Pipeline): \n",
    "            transformer = transformer_in_columns[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = transformer_in_columns[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError: # if no 'get_feature_names' function, use raw column name\n",
    "            names = raw_col_name\n",
    "        if isinstance(names,np.ndarray): # eg.\n",
    "            col_name += names.tolist()\n",
    "        elif isinstance(names,list):\n",
    "            col_name += names    \n",
    "        elif isinstance(names,str):\n",
    "            col_name.append(names)\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef = clf.named_steps['classifier'].coef_\n",
    "feature_names = get_column_names_from_ColumnTransformer(clf.named_steps['preprocessor'])\n",
    "for i in range(len(feature_names)):\n",
    "    if feature_names[i][0] == 'x':\n",
    "        flag = i\n",
    "        break\n",
    "cat_names = feature_names[flag:]\n",
    "select = []\n",
    "for i in range(coef.shape[1]):\n",
    "    if coef[0,i] != 0:\n",
    "        select.append(i)\n",
    "select_features = [feature_names[i] for i in select]\n",
    "len(select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21102"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(select_features)):\n",
    "    if select_features[i][0] == 'x':\n",
    "        flag = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_select = select_features[:flag]\n",
    "cat_select = select_features[flag:]\n",
    "inds = []\n",
    "for i in range(len(cat_select)):\n",
    "    inds.append(cat_names.index(cat_select[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data_scaler = StandardScaler()\n",
    "train_num = data_scaler.fit_transform(train_tiny_X[numCols])\n",
    "val_num = data_scaler.transform(val_tiny_X[numCols])\n",
    "train_num_df = pd.DataFrame(train_num,columns = numCols)[num_select]\n",
    "train_num_df = train_num_df.reset_index(drop=True)\n",
    "val_num_df = pd.DataFrame(val_num,columns = numCols)[num_select]\n",
    "val_num_df = val_num_df.reset_index(drop=True)\n",
    "\n",
    "train_cat = train_tiny_X[catCols]\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "one_hot.fit(train_cat)\n",
    "feature_names = [str(i) for i in list(one_hot.get_feature_names())]\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        train_piece = one_hot.transform(train_cat[i*1000:(i+1)*1000]).toarray()[:,inds]\n",
    "        val_piece = one_hot.transform(val_tiny_X[catCols]).toarray()[:,inds]\n",
    "    elif i == 3:\n",
    "        train_piece = np.vstack((train_piece,one_hot.transform(train_cat[3000:]).toarray()[:,inds]))      \n",
    "    else:\n",
    "        train_piece = np.vstack((train_piece,one_hot.transform(train_cat[i*1000:(i+1)*1000]).toarray()[:,inds]))\n",
    "\n",
    "train_cat_df = pd.DataFrame(train_piece,columns = cat_select)\n",
    "val_cat_df = pd.DataFrame(val_piece,columns = cat_select)\n",
    "newcatCols = list(train_cat_df.columns)\n",
    "\n",
    "try_train_X = pd.concat([train_num_df,train_cat_df],axis = 1)\n",
    "try_val_X = pd.concat([val_num_df,val_cat_df],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2942719</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4875524</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6589819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1172060</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2069395</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  target\n",
       "0     2942719     0.0\n",
       "1     4875524     0.0\n",
       "2     6589819     0.0\n",
       "3     1172060     1.0\n",
       "4     2069395     0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "train_tiny_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=0), param_distributions = random_grid, n_iter = 20, cv = 3, verbose=1, random_state=0)\n",
    "# rf_random.fit(try_train_X, train_tiny_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   iid='warn', n_iter=20, n_jobs=None,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [20, 73, 126, 180, 233,\n",
       "                                                         286, 340, 393, 446,\n",
       "                                                         500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(try_train_X, train_tiny_y['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 233, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 70, 'min_samples_leaf': 1, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.64      0.62       477\n",
      "         1.0       0.66      0.63      0.64       523\n",
      "\n",
      "    accuracy                           0.64      1000\n",
      "   macro avg       0.63      0.64      0.63      1000\n",
      "weighted avg       0.64      0.64      0.64      1000\n",
      "\n",
      "0.6351018755687033\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=233,random_state=0,min_samples_split=10,max_features='auto', max_depth=70,min_samples_leaf=1, bootstrap=False)\n",
    "clf.fit(try_train_X, train_tiny_y['target'])\n",
    "print(classification_report(val_tiny_y['target'], clf.predict(try_val_X)>0.5))\n",
    "print(roc_auc_score(val_tiny_y['target'], clf.predict(try_val_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest without Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mca_tiny_X = pd.read_csv('music_data/mca_train_tiny_X.csv')\n",
    "train_mca_tiny_y = pd.read_csv('music_data/mca_train_tiny_y.csv')\n",
    "\n",
    "val_mca_tiny_X = pd.read_csv('music_data/mca_val_tiny_X.csv')\n",
    "val_mca_tiny_y = pd.read_csv('music_data/mca_val_tiny_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mca_tiny_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-806e202e95ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                'bootstrap': bootstrap}\n\u001b[1;32m     20\u001b[0m \u001b[0mrf_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mca_tiny_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mca_tiny_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_mca_tiny_X' is not defined"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=0), param_distributions = random_grid, n_iter = 20, cv = 3, verbose=1, random_state=0)\n",
    "rf_random.fit(train_mca_tiny_X, train_mca_tiny_y['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 286, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 40, 'min_samples_leaf': 4, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 286\n",
      "building tree 2 of 286\n",
      "building tree 3 of 286\n",
      "building tree 4 of 286\n",
      "building tree 5 of 286\n",
      "building tree 6 of 286\n",
      "building tree 7 of 286\n",
      "building tree 8 of 286\n",
      "building tree 9 of 286\n",
      "building tree 10 of 286\n",
      "building tree 11 of 286\n",
      "building tree 12 of 286\n",
      "building tree 13 of 286\n",
      "building tree 14 of 286\n",
      "building tree 15 of 286\n",
      "building tree 16 of 286\n",
      "building tree 17 of 286\n",
      "building tree 18 of 286\n",
      "building tree 19 of 286\n",
      "building tree 20 of 286\n",
      "building tree 21 of 286\n",
      "building tree 22 of 286\n",
      "building tree 23 of 286\n",
      "building tree 24 of 286\n",
      "building tree 25 of 286\n",
      "building tree 26 of 286\n",
      "building tree 27 of 286\n",
      "building tree 28 of 286\n",
      "building tree 29 of 286\n",
      "building tree 30 of 286\n",
      "building tree 31 of 286\n",
      "building tree 32 of 286\n",
      "building tree 33 of 286\n",
      "building tree 34 of 286\n",
      "building tree 35 of 286\n",
      "building tree 36 of 286\n",
      "building tree 37 of 286\n",
      "building tree 38 of 286\n",
      "building tree 39 of 286\n",
      "building tree 40 of 286\n",
      "building tree 41 of 286\n",
      "building tree 42 of 286\n",
      "building tree 43 of 286\n",
      "building tree 44 of 286\n",
      "building tree 45 of 286\n",
      "building tree 46 of 286\n",
      "building tree 47 of 286\n",
      "building tree 48 of 286\n",
      "building tree 49 of 286\n",
      "building tree 50 of 286\n",
      "building tree 51 of 286\n",
      "building tree 52 of 286\n",
      "building tree 53 of 286\n",
      "building tree 54 of 286\n",
      "building tree 55 of 286\n",
      "building tree 56 of 286\n",
      "building tree 57 of 286\n",
      "building tree 58 of 286\n",
      "building tree 59 of 286\n",
      "building tree 60 of 286\n",
      "building tree 61 of 286\n",
      "building tree 62 of 286\n",
      "building tree 63 of 286\n",
      "building tree 64 of 286\n",
      "building tree 65 of 286\n",
      "building tree 66 of 286\n",
      "building tree 67 of 286\n",
      "building tree 68 of 286\n",
      "building tree 69 of 286\n",
      "building tree 70 of 286\n",
      "building tree 71 of 286\n",
      "building tree 72 of 286\n",
      "building tree 73 of 286\n",
      "building tree 74 of 286\n",
      "building tree 75 of 286\n",
      "building tree 76 of 286\n",
      "building tree 77 of 286\n",
      "building tree 78 of 286\n",
      "building tree 79 of 286\n",
      "building tree 80 of 286\n",
      "building tree 81 of 286\n",
      "building tree 82 of 286\n",
      "building tree 83 of 286\n",
      "building tree 84 of 286\n",
      "building tree 85 of 286\n",
      "building tree 86 of 286\n",
      "building tree 87 of 286\n",
      "building tree 88 of 286\n",
      "building tree 89 of 286\n",
      "building tree 90 of 286\n",
      "building tree 91 of 286\n",
      "building tree 92 of 286\n",
      "building tree 93 of 286\n",
      "building tree 94 of 286\n",
      "building tree 95 of 286\n",
      "building tree 96 of 286\n",
      "building tree 97 of 286\n",
      "building tree 98 of 286\n",
      "building tree 99 of 286\n",
      "building tree 100 of 286\n",
      "building tree 101 of 286\n",
      "building tree 102 of 286\n",
      "building tree 103 of 286\n",
      "building tree 104 of 286\n",
      "building tree 105 of 286\n",
      "building tree 106 of 286\n",
      "building tree 107 of 286\n",
      "building tree 108 of 286\n",
      "building tree 109 of 286\n",
      "building tree 110 of 286\n",
      "building tree 111 of 286\n",
      "building tree 112 of 286\n",
      "building tree 113 of 286\n",
      "building tree 114 of 286\n",
      "building tree 115 of 286\n",
      "building tree 116 of 286\n",
      "building tree 117 of 286\n",
      "building tree 118 of 286\n",
      "building tree 119 of 286\n",
      "building tree 120 of 286\n",
      "building tree 121 of 286\n",
      "building tree 122 of 286\n",
      "building tree 123 of 286\n",
      "building tree 124 of 286\n",
      "building tree 125 of 286\n",
      "building tree 126 of 286\n",
      "building tree 127 of 286\n",
      "building tree 128 of 286\n",
      "building tree 129 of 286\n",
      "building tree 130 of 286\n",
      "building tree 131 of 286\n",
      "building tree 132 of 286\n",
      "building tree 133 of 286\n",
      "building tree 134 of 286\n",
      "building tree 135 of 286\n",
      "building tree 136 of 286\n",
      "building tree 137 of 286\n",
      "building tree 138 of 286\n",
      "building tree 139 of 286\n",
      "building tree 140 of 286\n",
      "building tree 141 of 286\n",
      "building tree 142 of 286\n",
      "building tree 143 of 286\n",
      "building tree 144 of 286\n",
      "building tree 145 of 286\n",
      "building tree 146 of 286\n",
      "building tree 147 of 286\n",
      "building tree 148 of 286\n",
      "building tree 149 of 286\n",
      "building tree 150 of 286\n",
      "building tree 151 of 286\n",
      "building tree 152 of 286\n",
      "building tree 153 of 286\n",
      "building tree 154 of 286\n",
      "building tree 155 of 286\n",
      "building tree 156 of 286\n",
      "building tree 157 of 286\n",
      "building tree 158 of 286\n",
      "building tree 159 of 286\n",
      "building tree 160 of 286\n",
      "building tree 161 of 286\n",
      "building tree 162 of 286\n",
      "building tree 163 of 286\n",
      "building tree 164 of 286\n",
      "building tree 165 of 286\n",
      "building tree 166 of 286\n",
      "building tree 167 of 286\n",
      "building tree 168 of 286\n",
      "building tree 169 of 286\n",
      "building tree 170 of 286\n",
      "building tree 171 of 286\n",
      "building tree 172 of 286\n",
      "building tree 173 of 286\n",
      "building tree 174 of 286\n",
      "building tree 175 of 286\n",
      "building tree 176 of 286\n",
      "building tree 177 of 286\n",
      "building tree 178 of 286\n",
      "building tree 179 of 286\n",
      "building tree 180 of 286\n",
      "building tree 181 of 286\n",
      "building tree 182 of 286\n",
      "building tree 183 of 286\n",
      "building tree 184 of 286\n",
      "building tree 185 of 286\n",
      "building tree 186 of 286\n",
      "building tree 187 of 286\n",
      "building tree 188 of 286\n",
      "building tree 189 of 286\n",
      "building tree 190 of 286\n",
      "building tree 191 of 286\n",
      "building tree 192 of 286\n",
      "building tree 193 of 286\n",
      "building tree 194 of 286\n",
      "building tree 195 of 286\n",
      "building tree 196 of 286\n",
      "building tree 197 of 286\n",
      "building tree 198 of 286\n",
      "building tree 199 of 286\n",
      "building tree 200 of 286\n",
      "building tree 201 of 286\n",
      "building tree 202 of 286\n",
      "building tree 203 of 286\n",
      "building tree 204 of 286\n",
      "building tree 205 of 286\n",
      "building tree 206 of 286\n",
      "building tree 207 of 286\n",
      "building tree 208 of 286\n",
      "building tree 209 of 286\n",
      "building tree 210 of 286\n",
      "building tree 211 of 286\n",
      "building tree 212 of 286\n",
      "building tree 213 of 286\n",
      "building tree 214 of 286\n",
      "building tree 215 of 286\n",
      "building tree 216 of 286\n",
      "building tree 217 of 286\n",
      "building tree 218 of 286\n",
      "building tree 219 of 286\n",
      "building tree 220 of 286\n",
      "building tree 221 of 286\n",
      "building tree 222 of 286\n",
      "building tree 223 of 286\n",
      "building tree 224 of 286\n",
      "building tree 225 of 286\n",
      "building tree 226 of 286\n",
      "building tree 227 of 286\n",
      "building tree 228 of 286\n",
      "building tree 229 of 286\n",
      "building tree 230 of 286\n",
      "building tree 231 of 286\n",
      "building tree 232 of 286\n",
      "building tree 233 of 286\n",
      "building tree 234 of 286\n",
      "building tree 235 of 286\n",
      "building tree 236 of 286\n",
      "building tree 237 of 286\n",
      "building tree 238 of 286\n",
      "building tree 239 of 286\n",
      "building tree 240 of 286\n",
      "building tree 241 of 286\n",
      "building tree 242 of 286\n",
      "building tree 243 of 286\n",
      "building tree 244 of 286\n",
      "building tree 245 of 286\n",
      "building tree 246 of 286\n",
      "building tree 247 of 286\n",
      "building tree 248 of 286\n",
      "building tree 249 of 286\n",
      "building tree 250 of 286\n",
      "building tree 251 of 286\n",
      "building tree 252 of 286\n",
      "building tree 253 of 286\n",
      "building tree 254 of 286\n",
      "building tree 255 of 286\n",
      "building tree 256 of 286\n",
      "building tree 257 of 286\n",
      "building tree 258 of 286\n",
      "building tree 259 of 286\n",
      "building tree 260 of 286\n",
      "building tree 261 of 286\n",
      "building tree 262 of 286\n",
      "building tree 263 of 286\n",
      "building tree 264 of 286\n",
      "building tree 265 of 286\n",
      "building tree 266 of 286\n",
      "building tree 267 of 286\n",
      "building tree 268 of 286\n",
      "building tree 269 of 286\n",
      "building tree 270 of 286\n",
      "building tree 271 of 286\n",
      "building tree 272 of 286\n",
      "building tree 273 of 286\n",
      "building tree 274 of 286\n",
      "building tree 275 of 286\n",
      "building tree 276 of 286\n",
      "building tree 277 of 286\n",
      "building tree 278 of 286\n",
      "building tree 279 of 286\n",
      "building tree 280 of 286\n",
      "building tree 281 of 286\n",
      "building tree 282 of 286\n",
      "building tree 283 of 286\n",
      "building tree 284 of 286\n",
      "building tree 285 of 286\n",
      "building tree 286 of 286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.74      0.66       490\n",
      "         1.0       0.70      0.55      0.61       534\n",
      "\n",
      "    accuracy                           0.64      1024\n",
      "   macro avg       0.65      0.64      0.64      1024\n",
      "weighted avg       0.65      0.64      0.64      1024\n",
      "\n",
      "0.64364824581518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 286 out of 286 | elapsed:   10.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 286 out of 286 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 286 out of 286 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "clf_mca = RandomForestClassifier(verbose=2,n_estimators=286,random_state=0,min_samples_split=5,max_features='sqrt', max_depth=40,min_samples_leaf=4, bootstrap=True)\n",
    "clf_mca.fit(train_mca_tiny_X, train_mca_tiny_y['target'])\n",
    "print(classification_report(val_mca_tiny_y['target'], clf_mca.predict(val_mca_tiny_X)>0.5))\n",
    "print(roc_auc_score(val_mca_tiny_y['target'], clf_mca.predict(val_mca_tiny_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Up on the Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_X = pd.read_csv('new_data/new_train_X.csv')\n",
    "new_train_y = pd.read_csv('new_data/new_train_y.csv')\n",
    "\n",
    "new_valid_X = pd.read_csv('new_data/new_valid_X.csv')\n",
    "new_valid_y = pd.read_csv('new_data/new_valid_y.csv')\n",
    "\n",
    "new_test_X = pd.read_csv('new_data/new_test_X.csv')\n",
    "new_test_y = pd.read_csv('new_data/new_test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0     0.0\n",
       "1     1.0\n",
       "2     0.0\n",
       "3     1.0\n",
       "4     1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92122</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92123</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92124</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92125</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92126</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92127</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92128</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92129</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92130</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92131</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92132</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92133</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92134</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92135</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92136</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92137</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92138</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92139</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92140</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92141</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92142</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92143</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92144</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92145</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92146</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92147</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92148</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92149</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92150</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92151</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92152 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target\n",
       "0         1.0\n",
       "1         1.0\n",
       "2         0.0\n",
       "3         1.0\n",
       "4         1.0\n",
       "5         1.0\n",
       "6         0.0\n",
       "7         1.0\n",
       "8         0.0\n",
       "9         1.0\n",
       "10        1.0\n",
       "11        1.0\n",
       "12        0.0\n",
       "13        1.0\n",
       "14        0.0\n",
       "15        1.0\n",
       "16        0.0\n",
       "17        1.0\n",
       "18        1.0\n",
       "19        1.0\n",
       "20        1.0\n",
       "21        0.0\n",
       "22        1.0\n",
       "23        1.0\n",
       "24        1.0\n",
       "25        0.0\n",
       "26        0.0\n",
       "27        1.0\n",
       "28        0.0\n",
       "29        0.0\n",
       "...       ...\n",
       "92122     0.0\n",
       "92123     0.0\n",
       "92124     0.0\n",
       "92125     0.0\n",
       "92126     0.0\n",
       "92127     1.0\n",
       "92128     0.0\n",
       "92129     0.0\n",
       "92130     0.0\n",
       "92131     1.0\n",
       "92132     0.0\n",
       "92133     0.0\n",
       "92134     1.0\n",
       "92135     0.0\n",
       "92136     1.0\n",
       "92137     1.0\n",
       "92138     0.0\n",
       "92139     0.0\n",
       "92140     0.0\n",
       "92141     0.0\n",
       "92142     1.0\n",
       "92143     0.0\n",
       "92144     1.0\n",
       "92145     1.0\n",
       "92146     1.0\n",
       "92147     0.0\n",
       "92148     1.0\n",
       "92149     0.0\n",
       "92150     0.0\n",
       "92151     1.0\n",
       "\n",
       "[92152 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_y.rename(columns={'0.0':'target'})\n",
    "new_valid_y.rename(columns={'0.0':'target'})\n",
    "new_valid_y.rename(columns={'0.0':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3efc9938f8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mval_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_valid_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatCols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_piece\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_piece\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_scaler = StandardScaler()\n",
    "train_num = data_scaler.fit_transform(new_train_X[numCols])\n",
    "val_num = data_scaler.transform(new_valid_X[numCols])\n",
    "train_num_df = pd.DataFrame(train_num,columns = numCols)[num_select]\n",
    "train_num_df = train_num_df.reset_index(drop=True)\n",
    "val_num_df = pd.DataFrame(val_num,columns = numCols)[num_select]\n",
    "val_num_df = val_num_df.reset_index(drop=True)\n",
    "\n",
    "train_cat = new_train_X[catCols]\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "one_hot.fit(train_cat)\n",
    "feature_names = [str(i) for i in list(one_hot.get_feature_names())]\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        train_piece = one_hot.transform(train_cat[i*1000:(i+1)*1000]).toarray()[:,inds]\n",
    "        val_piece = one_hot.transform(new_valid_X[catCols]).toarray()[:,inds]\n",
    "    elif i == 3:\n",
    "        train_piece = np.vstack((train_piece,one_hot.transform(train_cat[3000:]).toarray()[:,inds]))      \n",
    "    else:\n",
    "        train_piece = np.vstack((train_piece,one_hot.transform(train_cat[i*1000:(i+1)*1000]).toarray()[:,inds]))\n",
    "\n",
    "train_cat_df = pd.DataFrame(train_piece,columns = cat_select)\n",
    "val_cat_df = pd.DataFrame(val_piece,columns = cat_select)\n",
    "newcatCols = list(train_cat_df.columns)\n",
    "\n",
    "new_train_X = pd.concat([train_num_df,train_cat_df],axis = 1)\n",
    "new_valid_X = pd.concat([val_num_df,val_cat_df],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(random_state=0), param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=0)\n",
    "# rf_random.fit(try_train_X, train_tiny_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
