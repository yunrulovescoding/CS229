{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import sklearn.datasets\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,classification_report,roc_auc_score\n",
    "import sklearn\n",
    "import prince\n",
    "\n",
    "import mca\n",
    "import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== catboost_results_cluster.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.79      0.78     45023\n",
      "         1.0       0.79      0.77      0.78     46757\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.780160002590915\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.78     45095\n",
      "         1.0       0.79      0.77      0.78     46685\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7792765283405663\n",
      "------ Feature Importance ------\n",
      "[ 0.69249275  9.39362615 23.50958754  9.45894704  7.80315045  3.86556479\n",
      " 12.88759232  1.21823341  1.88639708  7.07852164  0.56300667  0.77477691\n",
      "  5.40516911  0.79941537  0.73149508  0.38111232  0.55084617  4.9399306\n",
      "  1.37786951  1.95686375  2.17598379  2.54941752]\n",
      "\n",
      "\n",
      "====== catboost_results_rate01_depth6.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.79      0.78     45023\n",
      "         1.0       0.79      0.77      0.78     46757\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7791967416657036\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.78     45095\n",
      "         1.0       0.79      0.77      0.78     46685\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7785385613224058\n",
      "------ Feature Importance ------\n",
      "[ 0.75764566 10.64811069 23.62594844  8.96821491  5.67187291  4.47998959\n",
      " 14.65168015  1.0940659   2.85021661  6.82985268  0.95933938  1.69522827\n",
      "  6.74377709  0.93166837  0.88250102  0.3587887   0.63189755  4.57643811\n",
      "  1.50198227  2.14078169]\n",
      "\n",
      "\n",
      "====== cluster_lgb_results.npz.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.78      0.77     45023\n",
      "         1.0       0.78      0.77      0.77     46757\n",
      "\n",
      "    accuracy                           0.77     91780\n",
      "   macro avg       0.77      0.77      0.77     91780\n",
      "weighted avg       0.77      0.77      0.77     91780\n",
      "\n",
      "0.8524240333914714\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.77      0.77     45095\n",
      "         1.0       0.78      0.77      0.77     46685\n",
      "\n",
      "    accuracy                           0.77     91780\n",
      "   macro avg       0.77      0.77      0.77     91780\n",
      "weighted avg       0.77      0.77      0.77     91780\n",
      "\n",
      "0.8514524235206427\n",
      "------ Feature Importance ------\n",
      "[45222 58327 43298 38113  9679  4602  7986 40990 11352 35772 29928 22050\n",
      "  7685 15154 19970  5714  7006 18022 39013 34664 21828  4718]\n",
      "\n",
      "\n",
      "====== catboost_results_new_l2_100.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.77     45023\n",
      "         1.0       0.79      0.77      0.78     46757\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7773445616497644\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.77     45095\n",
      "         1.0       0.78      0.77      0.78     46685\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7759931620324867\n",
      "------ Feature Importance ------\n",
      "[ 0.17700394  8.05587172 25.76125242  9.29057845  7.5259475   6.28133743\n",
      " 14.08190013  0.55046515  2.51434869  8.15376162  0.4752288   1.50716156\n",
      "  6.72317153  0.37113563  0.35968423  0.23932699  0.23592051  5.60994593\n",
      "  0.82395253  1.26200527]\n",
      "\n",
      "\n",
      "====== catboost_results_new_l2_25.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.78     45023\n",
      "         1.0       0.79      0.77      0.78     46757\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7795175202686278\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.78     45095\n",
      "         1.0       0.79      0.77      0.78     46685\n",
      "\n",
      "    accuracy                           0.78     91780\n",
      "   macro avg       0.78      0.78      0.78     91780\n",
      "weighted avg       0.78      0.78      0.78     91780\n",
      "\n",
      "0.7782660367983276\n",
      "------ Feature Importance ------\n",
      "[ 0.65640177  8.34701311 24.10862086  8.51087673  7.86296878  5.06677669\n",
      " 12.95059228  1.21911654  2.65178958  7.18579929  0.71238932  1.90238667\n",
      "  7.10510219  0.79849497  0.79828967  0.43325571  0.60081406  5.66659787\n",
      "  1.45818624  1.96452767]\n",
      "\n",
      "\n",
      "====== cluster_lgb_results.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.78      0.77     45023\n",
      "         1.0       0.78      0.77      0.77     46757\n",
      "\n",
      "    accuracy                           0.77     91780\n",
      "   macro avg       0.77      0.77      0.77     91780\n",
      "weighted avg       0.77      0.77      0.77     91780\n",
      "\n",
      "0.8524240333914714\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.77      0.77     45095\n",
      "         1.0       0.78      0.77      0.77     46685\n",
      "\n",
      "    accuracy                           0.77     91780\n",
      "   macro avg       0.77      0.77      0.77     91780\n",
      "weighted avg       0.77      0.77      0.77     91780\n",
      "\n",
      "0.8514524235206427\n",
      "------ Feature Importance ------\n",
      "[45222 58327 43298 38113  9679  4602  7986 40990 11352 35772 29928 22050\n",
      "  7685 15154 19970  5714  7006 18022 39013 34664 21828  4718]\n",
      "\n",
      "\n",
      "====== lgb_latest_results.npz ======\n",
      "------ Validation ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.77      0.76     45023\n",
      "         1.0       0.78      0.75      0.76     46757\n",
      "\n",
      "    accuracy                           0.76     91780\n",
      "   macro avg       0.76      0.76      0.76     91780\n",
      "weighted avg       0.76      0.76      0.76     91780\n",
      "\n",
      "0.8429051716588799\n",
      "------ Test ------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.77      0.76     45095\n",
      "         1.0       0.77      0.75      0.76     46685\n",
      "\n",
      "    accuracy                           0.76     91780\n",
      "   macro avg       0.76      0.76      0.76     91780\n",
      "weighted avg       0.76      0.76      0.76     91780\n",
      "\n",
      "0.842823386084496\n",
      "------ Feature Importance ------\n",
      "[47850 60301  7545  6303  4201   854  2386 37644  2405  7971  4815  3092\n",
      "   902  3522  6865  9624   610 25203 33152 32911]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'output'\n",
    "files = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]\n",
    "for f in files:\n",
    "    path = os.path.join(output_dir, f)\n",
    "    if f.endswith('.npz'):\n",
    "        try:\n",
    "            content = np.load(path, allow_pickle=True)\n",
    "            print(\"====== {} ======\".format(f))            \n",
    "            print(\"------ Validation ------\")\n",
    "            val_target = content['val_target']\n",
    "            val_preds = content['val_preds']\n",
    "            print(classification_report(val_target, val_preds>0.5))\n",
    "            print(roc_auc_score(val_target, val_preds))\n",
    "            \n",
    "            print(\"------ Test ------\")\n",
    "            test_target = content['test_target']\n",
    "            test_preds = content['test_preds']\n",
    "            print(classification_report(test_target, test_preds>0.5))\n",
    "            print(roc_auc_score(test_target, test_preds))\n",
    "            \n",
    "            print(\"------ Feature Importance ------\")\n",
    "            imp = content['feature_importance']\n",
    "            print(imp)\n",
    "        except Exception as e:\n",
    "            print(\"{} : !!! Failure !!! - {}\".format(f, e))\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
